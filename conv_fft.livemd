<!-- livebook:{"persist_outputs":true} -->

# Convolution with Nx

```elixir
Mix.install(
  [
    {:nx, "~> 0.9.2"},
    {:scidata, "~> 0.1.11"},
    {:exla, "~> 0.9.2"},
    {:kino, "~> 0.14.2"},
    {:stb_image, "~> 0.6.9"},
    {:axon, "~> 0.7.0"},
    {:kino_vega_lite, "~> 0.1.11"},
    {:nx_signal, "~> 0.2.0"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## What is a convolution?

A pragmatic point-of-view: when you write a multiplication of two polynomials, you do a convolution.

Formely, a __convolution product__ is a mathematical operation that can be understood as a sum of "sliding products" between two tensors.

It is widely used in signal processing.

#### How do you compute it?

There is a formula that sums over the number of samples or points of the signal we consider.

$$(s\circledast f)[n] = \sum_{i=-N}^{N} s[i-n]f[i]$$

> you will notice that we "pad" the tensors. This means that we add "zeros" to the signal function in order to build an input that is zero outisde of the number of points ($N$ here). This is done below with `padding: :same` or in the polynomial product or in the image filtering.

#### Link with the Discrete Fourier Ttansform

Consider a signal $s(t)$ that we want to transform using a filtering signal $f(t)$.

The convolution product $(s \star f)(t)$ has a remarkable property when viewed in the frequency domain.

Denote as $\hat{s}(\omega)$ and $\hat{f}(\omega)$ the Discrete Fourier Transform of $s$ and $f$ respectively (the DFT is implemented as a Fast Fourier Transform). We have a remarkable relationship: a multiplication (term by term) in the frequency domain is a DFT of a (circular) convolution.

$$\hat{s}(\omega) \cdot \hat{f}(\omega) = \widehat{s\circledast f}(\omega)$$

The key insight is that convolution in the time or spatial domain becomes a multiplication in the frequency domain, providing methods for signal transformation and filtering.

For instance, to remove specific frequencies, one can construct a filter $\hat{f}$ that is:

* 1 for all frequencies except the target region, say $\omega<\omega_c$
* 0 within the frequencies to be removed, so when $\omega>\omega_c$

By taking the inverse Fourier transform of this filter,   $\mathcal{F}^{-1}(\hat{f})$, we can implement the desired frequency filtering. The inverse of such a "window" function is:

$$f(t)=\dfrac{sin(\omega_c t)}{\pi t}$$

Furthermore, you see that one way to compute a convolution is to pad correctly the tensors, take the FFTs, do a (term by term) multiplication and take the inverse Fourier transform of this product:

$$s\circledast f = \mathcal{F}^{-1}(\hat{s}\cdot \hat{f})$$

## How to use `Nx.conv`

You have the following parameters:

* padding
* shape
* stride
* permutation

The shape is expected to be: `{batch_size, channel, dim_1, ...}`

The stride describes how you want to slide along the axes: one-by-one are jumps.

The parameter "batch_size" is important when you have do convolve a coloured image.

#### Example with an image

For example, you have a tensor that represents an RGBA image (coloured). Its shape is typically:

`{h,w,c} = {256,256,4}`.

You may want to set 4 for the batch_size, to batch per colour layer. We will use a permutation on the tensor. How?

* you add a new dimension: `Nx.new_axis(t, 0)`. The new shape of the tensor is `{1, 256,256,4}`.
* you set the `permutation_input` to permute the tensor with `[3,0,1,2]` because the second dimension must be the channel. You understand why you had to add a "fake" dimension to respect the pattern:

batch_size=4, channels=1, dim_1 = 256, dim_2=256

#### Example with a vector

If `t = Nx.shape(t)= {n}`, you can simply use `t` as a input in a convolution by doing `Nx.reshape({1,1,n})`. This respects the pattern:

batch_size=1, channel=1, dim_1=n.

## Padding a tensor

Some examples on how to use the function `Nx.pad` with a 1D tensor to add numbers to a tensor.

```elixir
t = Nx.tensor([1,2,3])
one_zero_left = Nx.pad(t, 0, [{1,0,0}])
two_5_right = Nx.pad(t, 5, [{0,2,0}])
spaced_with_three_five_from_left = Nx.pad(t, 5, [{1, 0, 3}])

[t, one_zero_left, two_5_right, spaced_with_three_five_from_left] 
|> Enum.map(&Nx.to_list/1)
```

<!-- livebook:{"output":true} -->

```
[[1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 5, 5], [5, 1, 5, 5, 5, 2, 5, 5, 5, 3]]
```

With a 2D-tensor, you need to add an extra dimension. For example, take:

```elixir
m = Nx.tensor([[1,2],[3,4]])
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s32[2][2]
  EXLA.Backend<host:0, 0.1005765884.2792751123.45245>
  [
    [1, 2],
    [3, 4]
  ]
>
```

We will successfully:

* add zero-padding on the first and last row
* add zero-padding on the first column and last column
* "suround" the tensor with zeroes

```elixir
{Nx.pad(m, 0, [{1,1,0}, {0,0,0}]), Nx.pad(m, 0, [{0,0,0}, {1,1,0}]), 
  Nx.pad(m, 0, [{1,1,0}, {1,1,0}])}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   s32[4][2]
   EXLA.Backend<host:0, 0.1005765884.2792751120.44983>
   [
     [0, 0],
     [1, 2],
     [3, 4],
     [0, 0]
   ]
 >,
 #Nx.Tensor<
   s32[2][4]
   EXLA.Backend<host:0, 0.1005765884.2792751120.44985>
   [
     [0, 1, 2, 0],
     [0, 3, 4, 0]
   ]
 >,
 #Nx.Tensor<
   s32[4][4]
   EXLA.Backend<host:0, 0.1005765884.2792751120.44987>
   [
     [0, 0, 0, 0],
     [0, 1, 2, 0],
     [0, 3, 4, 0],
     [0, 0, 0, 0]
   ]
 >}
```

## Example of convolution and FFT

We use again our signal composed of two signals with frequency 5Hz (or period of 1/5s) and 20Hz (ie period of 1/20s).

$s=\sin(2 \pi 5t ) + \frac14\sin(2\pi 20 t)$

If we want to filter frequencies above $f_c=10$Hz,  our filter signal in the frequency domain can be:

$\hat{f}(\omega) = 1$ when $|\omega|<\omega_c = 2\pi f_c$

$\hat{f}(\omega) = 0$ when $|\omega|>\omega_c = 2\pi f_c$

Its "Fourier inverse" is:

$$\dfrac{\sin(2\pi f_c t)}{\pi t}$$

Let's verify that:

* if we take the FFT of our signal $s$ and multiply by our window function $\hat{f}$, and take its IFFT, that we have removed the perturbation at 20Hz,
* if we take the "standard" convolution and take its FFT, then this product should be free of perturbations at 20Hz, and the FFT transformation of this product should not contain a significant amplitude at 20Hz.

<!-- livebook:{"break_markdown":true} -->

#### Direct convolution in the time (or spatial) domain

We firstly evaluate the "direct" convolution. We plot the convolution of the signal with the filter.

```elixir
defmodule C do
  import Nx.Defn
  import Nx.Constants, only: [pi: 0]

  defn bins(opts) do
    start = opts[:start]
    end_point = opts[:end_point]
    fs = opts[:fs]
    Nx.linspace(start, end_point, n: fs, endpoint: true, type: :f32)
  end
  
  defn sample(t) do
    f1 = 5; f2 = 20;
    Nx.sin(2 * pi() * f1 * t ) + 1/4 * Nx.sin(2 * pi() * f2 * t)
  end
    
  defn filter(t, opts) do
    fc = opts[:fc]
    Nx.sin(2*pi()*fc*t) / (pi()*t)
  end

  def conv(s,f) do
    s = Nx.reshape(s, {1,1,Nx.size(s)})
    f = Nx.reshape(f, {1,1,Nx.size(f)})
    Nx.conv(s, Nx.reverse(f), padding: :same)
  end

  defn base(t) do
    f1 = 5;
    Nx.sin(2 * pi() * f1 * t )
  end
end

fs = 200
# center the curve for the FFT of the conv
opts = [start: -0.5, end_point: 0.5, fs: fs ]

x = C.bins(opts) 
y_signal = C.sample(x) 
fc = 10
y_filter = C.filter(x, fc: fc)

y_conv_s_f = C.conv(y_signal, y_filter) |> Nx.flatten()

signal = %{x: Nx.to_list(x), y: Nx.to_list(y_signal)}
#filter = %{x: Nx.to_list(x), y: Nx.to_list(y_filter)}
conv_s_f = %{x: Nx.to_list(x) , y: Nx.to_list(y_conv_s_f)}

VegaLite.new(width: 700, height: 400)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(signal, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative ,title: "time samples (s)")
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "signal amplitude"),
  VegaLite.new()
  |> VegaLite.data_from_values(conv_s_f, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "convolution"),
])
|> VegaLite.resolve(:scale, y: :independent)
```

In the plot above, we don't see any perturbation on top of the main period. The signal repeats itself 5 times during this 1s.

To confirm this, we plot the FFT of the convolution $(s\circledast f)(x)$.

It confirms that there is only one spike at 2Hz. This is what we wanted.

```elixir
n = 25
amplitudes = Nx.fft(y_conv_s_f) |> Nx.abs()

data_conv = %{
  x: (for i<- 0..n, do: i), 
  y: amplitudes[0..n] |> Nx.to_list() 
}

VegaLite.new(width: 700, height: 400)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(data_conv, only: ["x", "y"])
  |> VegaLite.mark(:bar)
  |> VegaLite.encode_field(:x, "x", type: :quantitative, title: "frequency bins")
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "Ampliude")
])
```

#### Filter in the frequency domain and reverse

We now work in the frequency domain with the Fourier transform $\hat{s}(\omega)$ of the signal.

We apply a filter $\hat{f}$  in the frequency domain. This function will be equal to 1 for $f<f_c=10$ and 0 elsewhere.

The product $\hat{s} \cdot \hat{f}$  is therefor equivalent to take the $f_c=10$ first elements of $\hat{s}$ and concatenante it with _zeros_.

We then take the _IFFT_ to return back to the time (or spatial) domain.

We see that we have removed the perturbation of frequency 20Hz as the base signal plot (at 5Hz, equivalently 5 periods per second) envelops nicely the IFFT values.

```elixir
n = 30; l = 10; fs = 200; start= -0.5; end_point = 0.5;
x = Nx.linspace(start, end_point, endpoint: false, n: fs, type: :f32)
s = C.sample(x)
fft = Nx.fft(s)


t_zeros = Nx.broadcast(Nx.complex(0,0), {n-l}) 
inverse = 
  Nx.concatenate([fft[0..l], t_zeros], axis: -1) 
  |> Nx.ifft() 
  |> Nx.real()
  

#-------- plot the IFFT
data = %{
  x: Nx.linspace(start, end_point, n: n+1, endpoint: false) |> Nx.to_list(),
  y: inverse[0..n] |> Nx.to_list()
}

#-------- fitting curve
xs = Nx.linspace(start, end_point, n: fs, endpoint: false)
signal = %{
  x: xs |> Nx.to_list(),
  y: C.base(xs) |> Nx.to_list()
}

VegaLite.new(height: 400, width: 700)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(data, only: ["x", "y"])
  |> VegaLite.mark(:bar)
  |> VegaLite.encode_field(:x, "x", type: :quantitative, title: "time(s)", scale: [domain: [-0.5, 0.5]])
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "amplitutde"),
  VegaLite.new()
  |> VegaLite.data_from_values(signal, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative, title: "time(s)", scale: [domain: [-0.5, 0.5]])
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "single signal"),
])
|> VegaLite.resolve(:scale, y: :independent)
```

## Example: polynomial multiplication using convolution

You can use the convolution product to compute the coefficients of the product of two polynomials.

<!-- livebook:{"break_markdown":true} -->

Lets visualize the convolution product.

Take two polynomials $P_1 = 1+X+X^2$ and $P_2=2+X$. There product is:

$$2+2X+3X^2+X^3+X^4$$

We calculate the convolution of $P_1=[1,1,1]$ with $P_2=[1,0,2]$ (the "kernel" is reversed here).

We are going to slide $P_2$ below $P_1$ and sum the term by term product with the rule that $P_2$ always overlaps $P_1$ by at least one term. This gives us 5 possibilities.

We therefor pad the first tensor with 2 zeros to the left and to the right, in order to calculate 5 sums of 3 products.

<!-- livebook:{"break_markdown":true} -->

```
t1           0 0 1 1 1 0 0
rev(t2)      1 0 2            0x1 + 0x0 + 1x2 = 2
               1 0 2          0x1 + 1x0 + 1x2 = 2
                 1 0 2        1x1 + 1x0 + 1x2 = 3
                   1 0 2      1x1 + 1x0 + 0x2 = 1
                     1 0 2    1x1 + 0x0 + 0x2 = 1

               2 2 3 1 1 
```

<!-- livebook:{"break_markdown":true} -->

We will apply a padding +2 "zeros" to the left and the right of the first tensor in the `Nx.conv` function.

```elixir
defmodule NxPoly do
  import Nx.Defn
  
  defn tprod(t1, t2) do
    t1 = Nx.stack(t1) 
    t1 = Nx.reshape(t1, {1,1, Nx.size(t1)})
    t2 = Nx.stack(t2)
    t2 = Nx.reshape(t2, {1,1, Nx.size(t2)})
    
    Nx.conv(t1, Nx.reverse(t2), padding: [{2,2}])
  end
end

```

<!-- livebook:{"output":true} -->

```
{:module, NxPoly, <<70, 79, 82, 49, 0, 0, 10, ...>>, true}
```

```elixir
NxPoly.tprod([1,1,1], [2,0,1])
|> Nx.as_type(:u8)
|> Nx.flatten()
|> Nx.to_list()
```

<!-- livebook:{"output":true} -->

```
[2, 2, 3, 1, 1]
```

We obtain the coefficients of the product: $2 + 2X + 3X^2 + X^3 + X^4$

## Example: edge detector with a discret differential kernel

The MNIST dataset contains images of numbers from 0 to 255 organized row wise with a format 28x28 pixels with 1 channel (grey levels).

This means that the pixel $(i,j)$ is at the $i$-th row and $ j$-th column.

```elixir
{{images_binary, type, images_shape} = _train_images, train_labels} = Scidata.MNIST.download_test()
{labels, {:u,8}, label_shape} = train_labels

images = images_binary |> Nx.from_binary(type) |> Nx.reshape(images_shape)

{{_nb, _channel, _height, _width}, _type}  = {images_shape, type}
```

<!-- livebook:{"output":true} -->

```
{{10000, 1, 28, 28}, {:u, 8}}
```

The labels describe what the image is. The 15th image is a "5".

```elixir
Nx.from_binary(labels, :u8)[15] |> Nx.to_number()
```

<!-- livebook:{"output":true} -->

```
5
```

To be sure, we display this image with `StbImage`.

> We transform the tensor as `StbImage` expects the format {height, width, channel} whilst the MNIST uses {channel, height, width}.

```elixir
five = images[15] 

Nx.transpose(five, axes: [1,2,0])
|> StbImage.from_nx()
|> StbImage.resize(500, 500)
```

<!-- livebook:{"output":true} -->

```
%StbImage{
  data: <<0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...>>,
  shape: {500, 500, 1},
  type: {:u, 8}
}
```

We can __detect edges__ by computing a discret derivation where the kernel will take the difference between adjacent pixel values. We can view this as a discret differential.

Because the image is slightly blurred, we will use a stride of 2: the kernel will be $ [-1, 0, 1]$. The convolution will compute a "sum_prod" of the image with these coefficients which take the difference between pixels separated by two units.

We can do this along the "width axis" (resp. "height axis") to detect vertical (resp horizontal) edges.

We known that the pixel coordinates are $(i,j)$ with $i$ the row and $j$ the column:

* vertical edges with horizontally adjacent pixels : $\mathrm{pix}(i, j) \cdot (-1) +  \mathrm{pix}(i, j+2) \cdot (1)$ and the kernel will be reshaped into $ {1,1,1,3}$.

* horizontal edges with vertically adjacent pixels: $\mathrm{pix}(i, j) \cdot (-1) +  \mathrm{pix}(i+2, j) \cdot (1)$ and the kernel will be reshaped into $ {1,1,3, 1}$

Below, we build the two transformed image by a vertical and horizontal "edge detector" and display them.

```elixir
edge_kernel = Nx.tensor([-1, 0, 1]) 

vertical_detection = Nx.conv(
  Nx.reshape(five, {1, 1, 28, 28}), 
  Nx.reshape(edge_kernel, {1, 1, 1, 3}), 
  padding: :same
)
  
horizontal_detection = Nx.conv(
  Nx.reshape(five, {1, 1, 28, 28}), 
   Nx.reshape(edge_kernel, {1, 1, 3, 1}), 
  padding: :same
)

Nx.concatenate([vertical_detection, horizontal_detection], axis: -1)
|> Nx.reshape({28, 56, 1}) 
|> Nx.as_type({:u, 8}) 
|> StbImage.from_nx()
|> StbImage.resize(500, 1000)
```

<!-- livebook:{"output":true} -->

```
%StbImage{
  data: <<0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...>>,
  shape: {500, 1000, 1},
  type: {:u, 8}
}
```

## Recover the kernel knowing the output

Suppose we have an image and its transformed image by a convolution.

We want to recover the convolution kernel "by hand" by running a gradient descent.

We will incrementally change the kernel in an iteration process.

The change of the kernel will be proportional to the gradient of a loss function we want to minimize.

The loss function will measure the difference between the known output ("vertical_convolution" here) and the current output image (given by `Nx.conv(input, current_kernel))`.

We will start from the null kernel. Notice we will use floats instead.

> Notice that we use a parameter "eps" to control the rate of change. This parameter is crucial for this algorithm. If it's to big, the iteration might be unstable, and if it's tool small, the iteration might be slow and/or converge to a local minimum.

```elixir
five = Nx.reshape(five, {1, 1, 28, 28})

kernel = Nx.tensor([-1,0,1]) |> Nx.reshape({1,1,1,3})

kernel_init = Nx.tensor([0.0, 0.0, 0.0]) |> Nx.reshape({1,1,1,3})

defmodule GradDesc do
  import Nx.Defn

  @learning_rate 1.0e-4
  @itermax 10

  defn curr_loss(input, kernel, out) do
    curr_out = Nx.conv(input, kernel, padding: :same)
    Nx.mean((curr_out - out)**2)
  end

  defn update(kernel, input, out) do
    der = grad(kernel, fn t -> curr_loss(input, t,  out) end)
    kernel - der * @learning_rate
  end

  defn loop(kernel, input, output) do
    {k,_, _,_} = 
      # for simplicity, we only set the number of steps to run.
      while {k = kernel, i=0, inp=input, out=output}, i <= @itermax  do
        {update(k, inp, out), i+1, inp, out}
      end
    {k, curr_loss(input, k,  output)}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, GradDesc, <<70, 79, 82, 49, 0, 0, 18, ...>>, true}
```

```elixir
IO.puts("Initial loss: #{round(Nx.to_number(GradDesc.curr_loss(five, kernel_init, vertical_detection)))}")

{k, loss} = GradDesc.loop(kernel_init, five, vertical_detection)

IO.puts("Loss after iterations: #{Nx.to_number(loss)}")
```

<!-- livebook:{"output":true} -->

```
Initial loss: 4647
Loss after iterations: 0.004979187157005072
```

<!-- livebook:{"output":true} -->

```
:ok
```

We expect to find a kernel close to $ [-1,0,1]$ after 10 iterations with a learning rate of 1e-4.

```elixir
Nx.to_list(k)|> List.flatten()
```

<!-- livebook:{"output":true} -->

```
[-0.9989625811576843, 2.4979462978080846e-6, 0.9989671111106873]
```

## Using an Axon model with a convolution layer to predict vertical edges

Let's use `Axon` to recover this kernel. Our model will be a simple convolution layer.

We will give to our model a serie of images and its transform (by the "known" kernel).

The first 15 images do not contain the number 5. We will pass them with the "vertical"-convolution transform to our model to "learn" how to detect vertical edges.

```elixir
# we produce a sample of 14 tuples {input, output = conv(input, kernel)} 
# which contain some numbers (different from 5)
l = 14
kernel = Nx.tensor([-1,0,1])  |> Nx.reshape({1,1,1,3}) 

training_data = 
  images[0..l] 
  |> Nx.reshape({l+1, 1,28,28})
  |> Nx.to_batched(1)
  |> Stream.map(fn img -> 
    {img, Nx.conv(img, kernel, padding: :same)} 
  end)
```

<!-- livebook:{"output":true} -->

```
#Stream<[
  enum: 0..14,
  funs: [#Function<50.105594673/1 in Stream.map/2>, #Function<50.105594673/1 in Stream.map/2>]
]>
```

The model is a simple convolution layer:

```elixir
model = 
  Axon.input("x", shape: {nil, 1, 28, 28})
  |> Axon.conv(1, 
    channels: :first, 
    padding: :same, 
    kernel_initializer: :zeros, 
    use_bias: false, 
    kernel_size: 3
  )
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"x" => {nil, 1, 28, 28}}
  outputs: "conv_0"
  nodes: 2
>
```

We train the model with the dataset

```elixir
optimizer = Polaris.Optimizers.adam(learning_rate: 1.0e-2)
optimizer = Polaris.Optimizers.adagrad(learning_rate: 1.0e-2)
optimizer = Polaris.Optimizers.adabelief(learning_rate: 1.0e-2)

loop = Axon.Loop.trainer(model, :mean_squared_error, optimizer)

params = Axon.Loop.run(loop, training_data, %{}, epochs: 20,  iterations: 50, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

14:44:36.739 [debug] Forwarding options: [compiler: EXLA] to JIT compiler

14:44:36.748 [warning] passing parameter map to initialization is deprecated, use %Axon.ModelState{} instead
Epoch: 0, Batch: 0, loss: 0.0000000
Epoch: 1, Batch: 0, loss: 4095.9575195
Epoch: 2, Batch: 0, loss: 2760.0224609
Epoch: 3, Batch: 0, loss: 2029.3372803
Epoch: 4, Batch: 0, loss: 1624.5377197
Epoch: 5, Batch: 0, loss: 1374.6849365
Epoch: 6, Batch: 0, loss: 1202.7775879
Epoch: 7, Batch: 0, loss: 1075.4449463
Epoch: 8, Batch: 0, loss: 976.2529907
Epoch: 9, Batch: 0, loss: 895.9296265
Epoch: 10, Batch: 0, loss: 828.9421387
Epoch: 11, Batch: 0, loss: 771.8013916
Epoch: 12, Batch: 0, loss: 722.1952515
Epoch: 13, Batch: 0, loss: 678.5291138
Epoch: 14, Batch: 0, loss: 639.6677246
Epoch: 15, Batch: 0, loss: 604.7778931
Epoch: 16, Batch: 0, loss: 573.2318115
Epoch: 17, Batch: 0, loss: 544.5447998
Epoch: 18, Batch: 0, loss: 518.3340454
Epoch: 19, Batch: 0, loss: 494.2913818
```

<!-- livebook:{"output":true} -->

```
#Axon.ModelState<
  Parameters: 9 (36 B)
  Trainable Parameters: 9 (36 B)
  Trainable State: 0, (0 B)
>
```

We can now check what our Axon model learnt. We compare:

* its "vertical"-convolution
* the predicted image by our model.

```elixir
five =  Nx.reshape(images[15], {1,1,28,28})

model_five = 
  Axon.predict(model, params, five)
  |> Nx.as_type(:u8)

conv_five = Nx.conv(
  Nx.reshape(images[15] , {1, 1, 28, 28}), 
  Nx.reshape(edge_kernel , {1, 1, 1, 3}), 
  padding: :same
)
|> Nx.as_type(:u8)

Nx.concatenate([model_five, conv_five], axis: -1)
|> Nx.reshape({28, 56, 1}) 
|> StbImage.from_nx()
|> StbImage.resize(500, 1500)
```

<!-- livebook:{"output":true} -->

```
%StbImage{
  data: <<0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...>>,
  shape: {500, 1500, 1},
  type: {:u, 8}
}
```

## Convolution live example

We want to apply a blurring" filter onto our webcam feed.

It works to convoluting the image by a kernel which will take the "gaussian" mean of the area of the image processed by the kernel.

Most tensor manipulations used below are described in this post:

<https://dockyard.com/blog/2022/03/15/nx-for-absolute-beginners>

#### The gaussian kernel

Let's see how the "blur" kernel is built.

* given a desired `kernel_size` and the mean `sigma`,
* it produces two vectors of integers `x` and `y` centered at 0 and of length the next odd number. For example, if the size is 4, it produces two vectors `[-2, -1, 0, 1, 2]`. If the size is 5, it produces the same two vectors (you want 0 to be "in the middle")
* you then use all the couples $(x_i,y_j)$ from these two vectors to compute the gaussian coefficients. For example with a size of 4, you will get 5x5=25 gaussian coefficients.
  $$
  \dfrac1{2\pi\sigma^2}\cdot \exp^{\dfrac{-x_i^2+y_j^2}{2\sigma}}
  $$
* you normalize them as their sum must be 1.

#### The convolution

We then run a convolution between the input (a tensor of an image) and this kernel.

Ths input is expected to be presented as `{height, width, channel}` with the type `:u8`, integers ranging from 0 to 255.
For example, a 256x256 colour PNG image will be have the shape `{256, 256, 4}`, with 4 layers of integers between 0 and 255 (plus some headers).

The image tensor will be transformed into floats in the range [0..1] simply by divided term by term by 255.

We want ot reduce the computations by computing one pixel out of two. This means instead of, for example an image of size 256x256, we reduce it to 128x128. This can be done using `strides: [2,2]`. You skip above one out of two row and column.

As said above, the convolution expects the format `{batch_size, channel, dim_1, dim_2,..}`.

We want to batch the computations _per colour_.  You therefor add a dimension to the tensors  with `Nx.axis_new`, and permute then to present the channel to be batched; from `{256,256,4}` to `{4, 1, 256, 256}`. THe permutation is done with the parameter `input_permutation`.

Once the convolution is done,

* you get rid of this extra "1" dimension with `Nx.squeeze`
* and rescale to 0..255, and then round thses floats to the nearest integer by casting into the type `:u8` as you need integers to produce an RGB image.

```elixir
# PValente filters

defmodule Filter do
  import Nx.Defn
  import Nx.Constants, only: [pi: 0]

  deftransform get_odd_size(opts) do
    size = opts[:kernel_size]
    size + 1 - rem(size, 2)
  end

  defn gaussian_blur_kernel(opts \\ []) do
    opts = keyword!(opts, [:kernel_size, :sigma])
    sigma = opts[:sigma]
    size = get_odd_size(opts)

    half_size = div(size, 2)

    range = {size} |> Nx.iota() |> Nx.subtract(half_size)

    x = Nx.vectorize(range, :x)
    y = Nx.vectorize(range, :y)

    # Apply Gaussian function to each element
    kernel =
      Nx.exp(-(x * x + y * y) / (2 * sigma * sigma))

    kernel = kernel / (2 * pi() * sigma * sigma)

    kernel = Nx.devectorize(kernel)

    # Normalize the kernel so the sum is 1
    kernel / Nx.sum(kernel)
  end

  defn apply_kernel(image, kernel, opts \\ []) do
    # you work on half of the image, reducing from 256 to 128 with strides: [2,2]
    opts = keyword!(opts, strides: [2, 2])

    # assumes channels are last in the image
    input_type = Nx.type(image)
    # use floats in [0..1] instead of u8 in [0..255]
    image = image / 255

    {m, n} = Nx.shape(kernel)
    kernel = Nx.reshape(kernel, {1,1,m,n})

    image
    # insert a new dimension: on the contrary to reshape, you don't need to use the current shape
    |> Nx.new_axis(0)
    |> Nx.conv(kernel,
      padding: :same,
      # swap {1, h, w, c} to {c, 1, h, w} for the calculations to be batched on each colour
      input_permutation: [3, 0, 1, 2],
      output_permutation: [3, 0, 1, 2],
      strides: opts[:strides]
    )
    |> Nx.squeeze(axes: [0])
    |> Nx.multiply(255)
    |> Nx.clip(0, 255)
    |> Nx.as_type(input_type)
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Filter, <<70, 79, 82, 49, 0, 0, 23, ...>>, true}
```

We can take a look into a kernel:

```elixir
Filter.gaussian_blur_kernel(kernel_size: 3, sigma: 0.5)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[x: 3][y: 3]
  EXLA.Backend<host:0, 0.1005765884.2792751120.48889>
  [
    [0.011343738064169884, 0.08381950855255127, 0.011343738064169884],
    [0.08381950855255127, 0.6193470358848572, 0.08381950855255127],
    [0.011343738064169884, 0.08381950855255127, 0.011343738064169884]
  ]
>
```

The code below in a custom `Kino.JS.Live` module. It displays the embedded webcam of your computer and produces a blurred copy of it by using the convolution module above.

The Javascript code captures the frames from the webcam into an `offScreenCanvas`. One frame of the webcam out of two will be convoluted. This frame is sent to the Elixir backend as a binary, as `Kino.JS.Live` makes this easy for us via a websocket with `pushEvent`.

The binary resulting of the convolution is converted into `:jpg` format as you have more compression than `:png`, typically a ratio of 1/4.

This binary is then sent back to the Javascript via a websocket with `broadcast_event` server-side and the  primitive `handleEvent` client-side.

To display the original size of 256x256 back (we reduced it to 128x128), use `canvas.drawImage` and specify the source dimensions and the target dimensions.

```elixir
defmodule Streamer do
  use Kino.JS
  use Kino.JS.Live

  def html() do
    """
    <video id= "invid"></video>
    <canvas id="canvas" width="256" height="256"></canvas>
    """
  end

  def start(), do: Kino.JS.Live.new(__MODULE__, html())

  @impl true
  def handle_connect(ctx), do: {:ok, ctx.assigns.html, ctx}

  @impl true
  def init(html, ctx), do: {:ok, assign(ctx, html: html)}

  # it receives a video frame in binary form
  @impl true
  def handle_event("new frame", {:binary,_, buffer}, ctx) do
    image = 
      Nx.from_binary(buffer, :u8) 
      # change shape from {1, 262144} to {256, 256, 4}
      |> Nx.reshape({256, 256, 4})

    # apply a {5,5} gaussian kernel
    kernel = Filter.gaussian_blur_kernel(kernel_size: 5, sigma: 1)   
    
    output = 
      Filter.apply_kernel(image, kernel) 
      |> StbImage.from_nx()
      |> StbImage.to_binary(:jpg)
    
    broadcast_event(ctx, "processed_frame", {:binary, %{}, output})
    {:noreply, ctx}
  end

  asset "main.js" do
    """
    export async function init(ctx, html) {
      ctx.root.innerHTML = html;
    
      const height = 256, width = 256,
        video = document.getElementById("invid"),
        renderCanvas = document.getElementById("canvas"),
        renderCanvasCtx = renderCanvas.getContext("2d"),
        offscreen = new OffscreenCanvas(width, height),
        offscreenCtx = offscreen.getContext("2d", { willReadFrequently: true });

      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { width, height },
        });
        video.srcObject = stream;
        let frameCounter = 0;
        video.play();
    
        const processFrame = async (_now, _metadata) => {
          frameCounter++;
          // process only 1 frame out of 2
          if (frameCounter % 2 == 0) {
            offscreenCtx.drawImage(video, 0, 0, width, height);
            const {data}  = offscreenCtx.getImageData(0, 0, width, height);
            ctx.pushEvent("new frame", [{}, data.buffer]);
          }
          requestAnimationFrame(processFrame);
        };
    
        requestAnimationFrame(processFrame);
      } catch (err) {
        console.error("Webcam access error:", err);
      }

      ctx.handleEvent("processed_frame", async ([{}, binary]) => {
        const bitmap = await createImageBitmap(new Blob([binary], { type: "image/jpeg" }));
        renderCanvasCtx.drawImage(bitmap, 0, 0, bitmap.width, bitmap.height, 0,0, 256,256)
      });
    }
    """
  end
end

```

<!-- livebook:{"output":true} -->

```
{:module, Streamer, <<70, 79, 82, 49, 0, 0, 17, ...>>, :ok}
```

```elixir
Streamer.start()
```

As a side note, the new [webGPU API](https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API) will be available, meaning the browser will be able to directly run these computations.

## Convolution and music

# Continuation...Using Convolution

```elixir
Mix.install(
  [
    {:nx, "~> 0.9.2"},
    {:scidata, "~> 0.1.11"},
    {:exla, "~> 0.9.2"},
    {:kino, "~> 0.14.2"},
    {:stb_image, "~> 0.6.9"},
    {:axon, "~> 0.7.0"},
    {:kino_vega_lite, "~> 0.1.11"},
    {:nx_signal, "~> 0.2.0"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## What is a convolution?

A pragmatic point-of-view: when you write a multiplication of two integers, you do a convolution!

We give another example further down  of a convolution;  when you multiply two polynomials, you do a convolution (it is basically the same idea).

Formely, a __convolution product__ is a mathematical operation that can be understood as a sum of "sliding products" between two tensors.

It is widely used in signal processing.

#### How do you compute it?

There is a formula that sums over the number of samples or points of the signal we consider.

$$(s\circledast f)[n] = \sum_{i=-N}^{N} s[i-n]f[i]$$

> you will notice that we "pad" the tensors. This means that we add "zeros" to the signal function in order to build an input that is zero outisde of the number of points ($N$ here). This is done below with `padding: :same` or in the polynomial product or in the image filtering.

#### Link with the Discrete Fourier Ttansform

Consider a signal $s(t)$ that we want to transform using a filtering signal $f(t)$.

The convolution product $(s \star f)(t)$ has a remarkable property when viewed in the frequency domain.

Denote as $\hat{s}(\omega)$ and $\hat{f}(\omega)$ the Discrete Fourier Transform of $s$ and $f$ respectively (the DFT is implemented as a Fast Fourier Transform). We have a remarkable relationship: a multiplication (term by term) in the frequency domain is a DFT of a (circular) convolution.

$$\hat{s}(\omega) \cdot \hat{f}(\omega) = \widehat{s\circledast f}(\omega)$$

The key insight is that convolution in the time or spatial domain becomes a multiplication in the frequency domain, providing methods for signal transformation and filtering.

For instance, to remove specific frequencies, one can construct a filter $\hat{f}$ that is:

* 1 for all frequencies except the target region, say $\omega<\omega_c$
* 0 within the frequencies to be removed, so when $\omega>\omega_c$

By taking the inverse Fourier transform of this filter,   $\mathcal{F}^{-1}(\hat{f})$, we can implement the desired frequency filtering. The inverse of such a "window" function is:

$$f(t)=\dfrac{sin(\omega_c t)}{\pi t}$$

Furthermore, you see that one way to compute a convolution is to pad correctly the tensors, take the FFTs, do a (term by term) multiplication and take the inverse Fourier transform of this product:

$$s\circledast f = \mathcal{F}^{-1}(\hat{s}\cdot \hat{f})$$

## Example of convolution and FFT

We use again our signal composed of two signals with frequency 5Hz (or period of 1/5s) and 20Hz (ie period of 1/20s).

$s=\sin(2 \pi 5t ) + \frac14\sin(2\pi 20 t)$

If we want to filter frequencies above $f_c=10$Hz,  our filter signal in the frequency domain can be:

$\hat{f}(\omega) = 1$ when $|\omega|<\omega_c = 2\pi f_c$

$\hat{f}(\omega) = 0$ when $|\omega|>\omega_c = 2\pi f_c$

Its "Fourier inverse" is:

$$\dfrac{\sin(2\pi f_c t)}{\pi t}$$

Let's verify that:

* if we take the FFT of our signal $s$ and multiply by our window function $\hat{f}$, and take its IFFT, that we have removed the perturbation at 20Hz,
* if we take the "standard" convolution and take its FFT, then this product should be free of perturbations at 20Hz, and the FFT transformation of this product should not contain a significant amplitude at 20Hz.

<!-- livebook:{"break_markdown":true} -->

#### Direct convolution in the time (or spatial) domain

We firstly evaluate the "direct" convolution. We plot the convolution of the signal with the filter.

```elixir
defmodule C do
  import Nx.Defn
  import Nx.Constants, only: [pi: 0]

  defn bins(opts) do
    start = opts[:start]
    end_point = opts[:end_point]
    fs = opts[:fs]
    Nx.linspace(start, end_point, n: fs, endpoint: true, type: :f32)
  end
  
  defn sample(t) do
    f1 = 5; f2 = 20;
    Nx.sin(2 * pi() * f1 * t ) + 1/4 * Nx.sin(2 * pi() * f2 * t)
  end
    
  defn filter(t, opts) do
    fc = opts[:fc]
    Nx.sin(2*pi()*fc*t) / (pi()*t)
  end

  def conv(s,f) do
    s = Nx.reshape(s, {1,1,Nx.size(s)})
    f = Nx.reshape(f, {1,1,Nx.size(f)})
    Nx.conv(s, Nx.reverse(f), padding: :same)
  end

  defn base(t) do
    f1 = 5;
    Nx.sin(2 * pi() * f1 * t )
  end
end

fs = 200
# center the curve for the FFT of the conv
opts = [start: -0.5, end_point: 0.5, fs: fs ]

x = C.bins(opts) 
y_signal = C.sample(x) 
fc = 10
y_filter = C.filter(x, fc: fc)

y_conv_s_f = C.conv(y_signal, y_filter) |> Nx.flatten()

signal = %{x: Nx.to_list(x), y: Nx.to_list(y_signal)}
#filter = %{x: Nx.to_list(x), y: Nx.to_list(y_filter)}
conv_s_f = %{x: Nx.to_list(x) , y: Nx.to_list(y_conv_s_f)}

VegaLite.new(width: 700, height: 400)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(signal, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative ,title: "time samples (s)")
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "signal amplitude"),
  VegaLite.new()
  |> VegaLite.data_from_values(conv_s_f, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "convolution"),
])
|> VegaLite.resolve(:scale, y: :independent)
```

In the plot above, we don't see any perturbation on top of the main period. The signal repeats itself 5 times during this 1s.

To confirm this, we plot the FFT of the convolution $(s\circledast f)(x)$.

It confirms that there is only one spike at 2Hz. This is what we wanted.

```elixir
n = 25
amplitudes = Nx.fft(y_conv_s_f) |> Nx.abs()

data_conv = %{
  x: (for i<- 0..n, do: i), 
  y: amplitudes[0..n] |> Nx.to_list() 
}

VegaLite.new(width: 700, height: 400)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(data_conv, only: ["x", "y"])
  |> VegaLite.mark(:bar)
  |> VegaLite.encode_field(:x, "x", type: :quantitative, title: "frequency bins")
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "Ampliude")
])
```

#### Filter in the frequency domain and reverse

We now work in the frequency domain with the Fourier transform $\hat{s}(\omega)$ of the signal.

We apply a filter $\hat{f}$  in the frequency domain. This function will be equal to 1 for $f<f_c=10$ and 0 elsewhere.

The product $\hat{s} \cdot \hat{f}$  is therefor equivalent to take the $f_c=10$ first elements of $\hat{s}$ and concatenante it with _zeros_.

We then take the _IFFT_ to return back to the time (or spatial) domain.

We see that we have removed the perturbation of frequency 20Hz as the base signal plot (at 5Hz, equivalently 5 periods per second) envelops nicely the IFFT values.

```elixir
n = 30; l = 10; fs = 200; start= -0.5; end_point = 0.5;
x = Nx.linspace(start, end_point, endpoint: false, n: fs, type: :f32)
s = C.sample(x)
fft = Nx.fft(s)

#data_fft = %{x: (for i <- 0..fs, do: i), y: Nx.to_list(Nx.abs(fft))}

t_zeros = Nx.broadcast(Nx.complex(0,0), {n-l}) 
inverse = 
  Nx.concatenate([fft[0..l], t_zeros], axis: -1) 
  #fft[0..]
  |> Nx.ifft() 
  |> Nx.real()
  

# plot the IFFT
data = %{
  x: Nx.linspace(start, end_point, n: n+1, endpoint: false) |> Nx.to_list(),
  y: inverse[0..n] |> Nx.to_list()
}

# fitting curve
xs = Nx.linspace(start, end_point, n: fs, endpoint: false)
signal = %{
  x: xs |> Nx.to_list(),
  y: C.base(xs) |> Nx.to_list()
}

VegaLite.new(height: 400, width: 700)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(data, only: ["x", "y"])
  |> VegaLite.mark(:bar)
  |> VegaLite.encode_field(:x, "x", type: :quantitative, title: "time(s)", scale: [domain: [-0.5, 0.5]])
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "amplitutde"),
  VegaLite.new()
  |> VegaLite.data_from_values(signal, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative, title: "time(s)", scale: [domain: [-0.5, 0.5]])
  |> VegaLite.encode_field(:y, "y", type: :quantitative, title: "single signal"),
])
|> VegaLite.resolve(:scale, y: :independent)
```

## Padding a tensor

Some examples on how to use the function `Nx.pad` with a 1D tensor to add numbers to a tensor.

```elixir
t = Nx.tensor([1,2,3])
one_zero_left = Nx.pad(t, 0, [{1,0,0}])
two_5_right = Nx.pad(t, 5, [{0,2,0}])
spaced_with_three_five_from_left = Nx.pad(t, 5, [{1, 0, 3}])

[t, one_zero_left, two_5_right, spaced_with_three_five_from_left] 
|> Enum.map(&Nx.to_list/1)
```

With a 2D-tensor, you need to add an extra dimension. For example, take:

```elixir
m = Nx.tensor([[1,2],[3,4]])
```

We will successfully:

* add zero-padding on the first and last row
* add zero-padding on the first column and last column
* "suround" the tensor with zeroes

```elixir
{Nx.pad(m, 0, [{1,1,0}, {0,0,0}]), Nx.pad(m, 0, [{0,0,0}, {1,1,0}]), 
  Nx.pad(m, 0, [{1,1,0}, {1,1,0}])}
```

## Example: polynomial multiplication using convolution

You can use the convolution product to compute the coefficients of the product of two polynomials.

<!-- livebook:{"break_markdown":true} -->

Lets visualize the convolution product.

Take two polynomials $P_1 = 1+X+X^2$ and $P_2=2+X$. There product is:

$$2+2X+3X^2+X^3+X^4$$

We calculate the convolution of $P_1=[1,1,1]$ with $P_2=[1,0,2]$ (the "kernel" is reversed here).

We are going to slide $P_2$ below $P_1$ and sum the term by term product with the rule that $P_2$ always overlaps $P_1$ by at least one term. This gives us 5 possibilities.

We therefor pad the first tensor with 2 zeros to the left and to the right, in order to calculate 5 sums of 3 products.

<!-- livebook:{"break_markdown":true} -->

```
t1           0 0 1 1 1 0 0
rev(t2)      1 0 2            0x1 + 0x0 + 1x2 = 2
               1 0 2          0x1 + 1x0 + 1x2 = 2
                 1 0 2        1x1 + 1x0 + 1x2 = 3
                   1 0 2      1x1 + 1x0 + 0x2 = 1
                     1 0 2    1x1 + 0x0 + 0x2 = 1

               2 2 3 1 1 
```

<!-- livebook:{"break_markdown":true} -->

We will apply a padding +2 "zeros" to the left and the right of the first tensor in the `Nx.conv` function.

```elixir
defmodule NxPoly do
  import Nx.Defn
  
  defn tprod(t1, t2) do
    t1 = Nx.stack(t1) 
    t1 = Nx.reshape(t1, {1,1, Nx.size(t1)})
    t2 = Nx.stack(t2)
    t2 = Nx.reshape(t2, {1,1, Nx.size(t2)})
    
    Nx.conv(t1, Nx.reverse(t2), padding: [{2,2}])
  end
end

```

```elixir
NxPoly.tprod([1,1,1], [2,0,1])
|> Nx.as_type(:u8)
|> Nx.flatten()
|> Nx.to_list()
```

We obtain the coefficients of the product: $2 + 2X + 3X^2 + X^3 + X^4$

## Example: edge detector with a discret differential kernel

The MNIST dataset contains images of numbers from 0 to 255 organized row wise with a format 28x28 pixels with 1 channel (grey levels).

This means that the pixel $(i,j)$ is at the $i$-th row and $ j$-th column.

```elixir
{{images_binary, type, images_shape} = _train_images, train_labels} = Scidata.MNIST.download_test()
{labels, {:u,8}, label_shape} = train_labels

images = images_binary |> Nx.from_binary(type) |> Nx.reshape(images_shape)

{{_nb, _channel, _height, _width}, _type}  = {images_shape, type}
```

The labels describe what the image is. The 15th image is a "5".

```elixir
Nx.from_binary(labels, :u8)[15] |> Nx.to_number()
```

To be sure, we display this image with `StbImage`.

> We transform the tensor as `StbImage` expects the format {height, width, channel} whilst the MNIST uses {channel, height, width}.

```elixir
five = images[15] 

Nx.transpose(five, axes: [1,2,0])
|> StbImage.from_nx()
|> StbImage.resize(500, 500)
```

We can __detect edges__ by computing a discret derivation where the kernel will take the difference between adjacent pixel values. We can view this as a discret differential.

Because the image is slightly blurred, we will use a stride of 2: the kernel will be $ [-1, 0, 1]$. The convolution will compute a "sum_prod" of the image with these coefficients which take the difference between pixels separated by two units.

We can do this along the "width axis" (resp. "height axis") to detect vertical (resp horizontal) edges.

We known that the pixel coordinates are $(i,j)$ with $i$ the row and $j$ the column:

* vertical edges with horizontally adjacent pixels : $\mathrm{pix}(i, j) \cdot (-1) +  \mathrm{pix}(i, j+2) \cdot (1)$ and the kernel will be reshaped into $ {1,1,1,3}$.

* horizontal edges with vertically adjacent pixels: $\mathrm{pix}(i, j) \cdot (-1) +  \mathrm{pix}(i+2, j) \cdot (1)$ and the kernel will be reshaped into $ {1,1,3, 1}$

Below, we build the two transformed image by a vertical and horizontal "edge detector" and display them.

```elixir
edge_kernel = Nx.tensor([-1, 0, 1]) 

vertical_detection = Nx.conv(
  Nx.reshape(five, {1, 1, 28, 28}), 
  Nx.reshape(edge_kernel, {1, 1, 1, 3}), 
  padding: :same
)
  
horizontal_detection = Nx.conv(
  Nx.reshape(five, {1, 1, 28, 28}), 
   Nx.reshape(edge_kernel, {1, 1, 3, 1}), 
  padding: :same
)

Nx.concatenate([vertical_detection, horizontal_detection], axis: -1)
|> Nx.reshape({28, 56, 1}) 
|> Nx.as_type({:u, 8}) 
|> StbImage.from_nx()
|> StbImage.resize(500, 1000)
```

## Recover the kernel knowing the output

Suppose we have an image and its transformed image by a convolution.

We want to recover the convolution kernel "by hand" by running a gradient descent.

We will incrementally change the kernel in an iteration process.

The change of the kernel will be proportional to the gradient of a loss function we want to minimize.

The loss function will measure the difference between the known output ("vertical_convolution" here) and the current output image (given by `Nx.conv(input, current_kernel))`.

We will start from the null kernel. Notice we will use floats instead.

> Notice that we use a parameter "eps" to control the rate of change. This parameter is crucial for this algorithm. If it's to big, the iteration might be unstable, and if it's tool small, the iteration might be slow and/or converge to a local minimum.

```elixir
five = Nx.reshape(five, {1, 1, 28, 28})

kernel = Nx.tensor([-1,0,1]) |> Nx.reshape({1,1,1,3})

kernel_init = Nx.tensor([0.0, 0.0, 0.0]) |> Nx.reshape({1,1,1,3})

defmodule GradDesc do
  import Nx.Defn

  @learning_rate 1.0e-4
  @itermax 10

  defn curr_loss(input, kernel, out) do
    curr_out = Nx.conv(input, kernel, padding: :same)
    Nx.mean((curr_out - out)**2)
  end

  defn update(kernel, input, out) do
    der = grad(kernel, fn t -> curr_loss(input, t,  out) end)
    kernel - der * @learning_rate
  end

  defn loop(kernel, input, output) do
    {k,_, _,_} = 
      # for simplicity, we only set the number of steps to run.
      while {k = kernel, i=0, inp=input, out=output}, i <= @itermax  do
        {update(k, inp, out), i+1, inp, out}
      end
    {k, curr_loss(input, k,  output)}
  end
end
```

```elixir
IO.puts("Initial loss: #{round(Nx.to_number(GradDesc.curr_loss(five, kernel_init, vertical_detection)))}")

{k, loss} = GradDesc.loop(kernel_init, five, vertical_detection)

IO.puts("Loss after iterations: #{Nx.to_number(loss)}")
```

We expect to find a kernel close to $ [-1,0,1]$ after 10 iterations with a learning rate of 1e-4.

```elixir
IO.puts("Found kernel: #{inspect(Nx.to_list(k)|> List.flatten())}")
```

## Using an Axon model with a convolution layer to predict vertical edges

Let's use `Axon` to recover this kernel. Our model will be a simple convolution layer.

We will give to our model a serie of images and its transform (by the "known" kernel).

The first 15 images do not contain the number 5. We will pass them with the "vertical"-convolution transform to our model to "learn" how to detect vertical edges.

```elixir
# we produce a sample of 14 tuples {input, output = conv(input, kernel)} 
# which contain some numbers (different from 5)
l = 14
kernel = Nx.tensor([-1,0,1])  |> Nx.reshape({1,1,1,3}) 

training_data = 
  images[0..l] 
  |> Nx.reshape({l+1, 1,28,28})
  |> Nx.to_batched(1)
  |> Stream.map(fn img -> 
    {img, Nx.conv(img, kernel, padding: :same)} 
  end)
```

The model is a simple convolution layer:

```elixir
model = 
  Axon.input("x", shape: {nil, 1, 28, 28})
  |> Axon.conv(1, 
    channels: :first, 
    padding: :same, 
    kernel_initializer: :zeros, 
    use_bias: false, 
    kernel_size: 3
  )
```

We train the model with the dataset

```elixir
optimizer = Polaris.Optimizers.adam(learning_rate: 1.0e-2)
optimizer = Polaris.Optimizers.adagrad(learning_rate: 1.0e-2)
optimizer = Polaris.Optimizers.adabelief(learning_rate: 1.0e-2)

loop = Axon.Loop.trainer(model, :mean_squared_error, optimizer)

params = Axon.Loop.run(loop, training_data, %{}, epochs: 20,  iterations: 50, compiler: EXLA)
```

We can now check what our Axon model learnt. We compare:

* its "vertical"-convolution
* the predicted image by our model.

```elixir
five =  Nx.reshape(images[15], {1,1,28,28})

model_five = 
  Axon.predict(model, params, five)
  |> Nx.as_type(:u8)

conv_five = Nx.conv(
  Nx.reshape(images[15] , {1, 1, 28, 28}), 
  Nx.reshape(edge_kernel , {1, 1, 1, 3}), 
  padding: :same
)
|> Nx.as_type(:u8)

Nx.concatenate([model_five, conv_five], axis: -1)
|> Nx.reshape({28, 56, 1}) 
|> StbImage.from_nx()
|> StbImage.resize(500, 1500)
```

## Convolution live example

We want to apply a blurring" filter onto our webcam feed.

It works to convoluting the image by a kernel which will take the "gaussian" mean of the area of the image processed by the kernel.

```elixir
# PValente filters

defmodule Filter do
  import Nx.Defn

  deftransform get_odd_size(opts) do
    size = opts[:kernel_size]
    size + 1 - rem(size, 2)
  end

  defn gaussian_blur_kernel(opts \\ []) do
    opts = keyword!(opts, [:kernel_size, :sigma])
    sigma = opts[:sigma]
    size = get_odd_size(opts)

    half_size = div(size, 2)

    range = {size} |> Nx.iota() |> Nx.subtract(half_size)

    x = Nx.vectorize(range, :x)
    y = Nx.vectorize(range, :y)

    # Apply Gaussian function to each element
    kernel =
      Nx.exp(-(x * x + y * y) / (2 * sigma * sigma))

    kernel = kernel / (2 * Nx.Constants.pi() * sigma * sigma)

    kernel = Nx.devectorize(kernel)

    # Normalize the kernel so the sum is 1
    kernel / Nx.sum(kernel)
  end

  defn apply_kernel(image, kernel, opts \\ []) do
    opts = keyword!(opts, strides: [1, 1])

    # assumes channels are last in the image
    input_type = Nx.type(image)
    # use floats in [0..1] instead of u8 in [0..255]
    image = image / 255

    {m, n} = Nx.shape(kernel)
    kernel = Nx.reshape(kernel, {1,1,m,n})

    image
    # insert a new dimension: on the contrary to reshape, you don't need to use the current shape
    |> Nx.new_axis(0)
    |> Nx.conv(kernel,
      padding: :same,
      # swap {h, w,c} to {c, h,w} for the calculations to be batched on each colour
      input_permutation: [3, 0, 1, 2],
      output_permutation: [3, 0, 1, 2],
      strides: opts[:strides]
    )
    |> Nx.squeeze(axes: [0])
    |> Nx.multiply(255)
    |> Nx.clip(0, 255)
    |> Nx.as_type(input_type)
  end
end
```

This module run the embedded webcam and produces a blurred copy of it.

It uses the convolution module above to blur each frame.

```elixir
defmodule Streamer do
  use Kino.JS
  use Kino.JS.Live

  def html() do
    """
    <video id= "invid"></video>
    <canvas id="canvas" width="256" height="256"></canvas>
    """
  end

  def start(), do: Kino.JS.Live.new(__MODULE__, html())

  @impl true
  def handle_connect(ctx), do: {:ok, ctx.assigns.html, ctx}

  @impl true
  def init(html, ctx), do: {:ok, assign(ctx, html: html)}

  # it receives a video frame in binary form
  @impl true
  def handle_event("new frame", {:binary,_, buffer}, ctx) do
    image = 
      Nx.from_binary(buffer, :u8) 
      |> Nx.reshape({256, 256, 4})
    # image has shape {256, 256, 4}

    # apply a {5,5} gaussian kernel
    kernel = Filter.gaussian_blur_kernel(kernel_size: 5, sigma: 1)   
    
    output = 
      Filter.apply_kernel(image, kernel) 
      |> StbImage.from_nx()
      |> StbImage.to_binary(:png)
    
    broadcast_event(ctx, "processed_frame", {:binary, %{}, output})
    {:noreply, ctx}
  end

  asset "main.js" do
    """
    export async function init(ctx, html) {
      ctx.root.innerHTML = html;
    
      const height = 256, width = 256,
        video = document.getElementById("invid"),
        renderCanvas = document.getElementById("canvas"),
        renderCanvasCtx = canvas.getContext("2d"),
        offscreen = new OffscreenCanvas(width, height),
        offscreenCtx = offscreen.getContext("2d");

      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { width, height, frameRate: 30 },
        });
        video.srcObject = stream;
        video.addEventListener("loadedmetadata", () => {
          video.play();
          video.requestVideoFrameCallback(processFrame);
        });
    
        const processFrame = async (now, metadata) => {
          offscreenCtx.drawImage(video, 0, 0, width, height);
          const imageData = offscreenCtx.getImageData(0, 0, width, height);
          const rawData = imageData.data;
          ctx.pushEvent("new frame", [{}, rawData.buffer]);
          video.requestVideoFrameCallback(processFrame);
        };
      } catch (err) {
        console.error("Webcam access error:", err);
      }

      function rawDataToPng(binary, width, height) {
        const imageData = renderCanvasCtx.createImageData(width, height);
        imageData.data.set(new Uint8ClampedArray(binary));
        renderCanvasCtx.putImageData(imageData, 0, 0);
    
        return new Promise((resolve) => {
          renderCanvas.toBlob((blob) => resolve(blob), "image/png");
        });
      }

      ctx.handleEvent("processed_frame", async ([{}, binary]) => {
        await createImageBitmap(new Blob([binary], { type: "image/png" })).then(
          (bitmap) => renderCanvasCtx.drawImage(bitmap, 0, 0)
        );
      });
    }
    """
  end
end
```

```elixir
Streamer.start()
```

## Convolution and music
